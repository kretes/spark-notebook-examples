{
  "metadata" : {
    "name" : "BigSparkDemo",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.joda.time.DateTime\ntype UserId = String\n  type EventType = String\n\n  case class UserEvent(userId: UserId, eventType: EventType, dateTime: DateTime) {\n    def isPageView = eventType == \"P\"\n  }\n\n  def parseToUserEvent(line: String) = {\n    val split: Array[String] = line.split(\",\")\n    split match {\n      case Array(userId, eventType, dateTime) => UserEvent(userId, eventType, DateTime.parse(dateTime))\n    }\n  }\nsparkContext.textFile(\"/data/userEventsBig\")\n    .map(parseToUserEvent)\n    .filter(_.isPageView)\n    .groupBy(_.userId)   \n    .sortBy(_._2.size,false)\n    .map { case (userId, userEvents) => \n          s\"$userId,${userEvents.size}\" }\n.take(5).foreach(println)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}